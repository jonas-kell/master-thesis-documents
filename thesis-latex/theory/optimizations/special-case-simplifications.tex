Generally, handling the computation of the difference of the effective Hamiltonian for two states $\HeffOft[\vphantom{N}\smash{\tilde{N}}]-\HeffOft[N]$ is the most expensive computation that is required up to this point.
For one it is e.g. required in \autoref{eq:form-heff-difference} for the calculation of the spin-polarized kinetics.
Furthermore - as stated in \fullref{sec:theory-optimizations-monte-carlo} - it is also needed to compute the transition probability between two sampled states in \autoref{eq:transition-probability-final}.
The calculation of $\HeffOft[N]$ on its own requires an effort of \bigo{\text{\#}(\text{sites}) \cdot \text{\#}(\text{nearest neighbors})}.
However, for states \ketN[N] and \ketN[{\vphantom{N}\smash{\tilde{N}}}] that are connected with only small changes, most of the elements in the sum cancel and the complexity may here even be reduced to \bigo{$\text{\#}(\text{nearest neighbors})$}.
This is quite attractive, because it de-couples the per-step computational costs from the number of lattice sites - a vital step for simulating large systems efficiency - however this requires extra analytical calculations.

In this section analytical simplifications are presented to get the objects $E_0(N)-E_0(\vphantom{N}\smash{\tilde{N}})$ and $\HNOft - \HNOft[\vphantom{N}\smash{\tilde{N}}]$ (Caution, do not miss the extra \emph{minus} required, to calculate the required $\HeffOft[\vphantom{N}\smash{\tilde{N}}]-\HeffOft[N]$ from this!).

Evidently, this also needs to hold for the variational versions of the effective Hamiltonian, that were introduced in \autoref{sec:theory-variational-classical-networks-time-dependency}.
As the $\Phi_\ast(N)$ are inspired by the form of the base-energy-terms in $E_0(N)$ or parts of $\HNOft$, all the same arguments apply: 
$\Heffvcn{\vphantom{N}\smash{\tilde{N}}}{\vec{\eta}} - \Heffvcn{N}{\vec{\eta}} = \lsum[l] \eta_l \cdot \left[\Phi_l(\vphantom{N}\smash{\tilde{N}}) - \Phi_l(N)\right]$.
Yet all of the re-writes must be done precisely to produce the correct results.

\subsubsection*{Initial State}

The initial state of the system before the start of the time-evolution is encoded in $\psiN$, defined by \autoref{eq:base-expansion-state}. Different configurations can be considered here.
The best choice for this would logically be the one that save the most computational effort and provides the biggest symmetry for the terms to promote the possibility of terms canceling.

A choice that fulfills these requirements trivially, would be the perfectly uniform distribution of probability for all the base-states.
For most of the sources that describe similar methods (like e.g. \cite{isingDynamicsWithClassicalNetworks} or \cite{variationalClassicalNetworksPaper}, where the latter describes how the approach also works for other initial states) this initial state has been chosen for its advantages.

The calculation of the uniform probability is straight-forward and listed in \autoref{eq:psi-n-homogenous}, while the state would be written down like \autoref{eq:psi-zero}.

\begin{equation}
    \label{eq:psi-n-homogenous}
    \psiN{} = \frac{1}{\sqrt{\text{\#}(\text{states})}} \stackrel{\ref{eq:system-size}}{=} \frac{1}{\sqrt{2^{\text{\#}(\text{sites}) \cdot 2}}} = \frac{1}{2^{\text{\#}(\text{sites})}}
\end{equation}

\begin{equation}
    \label{eq:psi-zero}
    \ketpsiof[\schroedingerPicture]{t=0} = \bigotimes\limits_{l=1}^{\text{\#}(\text{states})} \frac{1}{2} \left( 1 + \withspinhcop[\schroedingerPicture]{l}{\up}{\dagger} + \withspinhcop[\schroedingerPicture]{l}{\down}{\dagger} + \withspinhcop[\schroedingerPicture]{l}{\up}{\dagger}  \withspinhcop[\schroedingerPicture]{l}{\down}{\dagger} \right) \ketN[0]
\end{equation}

Central advantage of the uniform distribution of base-state-probabilities is, that all $\psiN$ are equal, which makes it possible to cancel most terms from the sums, as $\psiN{}/\psiN[\vphantom{N}\smash{\tilde{N}}]{} = 1$.
The following two optimizations require this assumption.

As the states under consideration are \ketN[N] and \ketN[{\vphantom{N}\smash{\tilde{N}}}], the occupation-numbers $n_{l,\,\sigma}$ will describe the occupation of \ketN[N] and $\tilde{n}_{l,\,\sigma}$ the occupation of \ketN[{\vphantom{N}\smash{\tilde{N}}}].
Both of the following modifications restrict the values $\tilde{n}_{l,\,\sigma}$ to make them dependent on $n_{l,\,\sigma}$.

\subsubsection*{Single Flip Modification}

In this simplification, the maximum difference that can happen, is that a \emph{flipping} event occurs on site $i$ and spin $\sigma_i$. 
In the language of hard-core bosons this means one occupation turning form a 0 into a 1 or the inverse of that.
This operation doesn't conserve the number of particles, so depending on the application this might not be a desired modification for a Monte-Carlo-step.
Still, the calculation of the reduced density-matrix as described in \autoref{sec:theory-observables-density-matrix} requires calculation of the effective Hamiltonian difference between two states connected by such a modification.

The flipping results in the values for $\tilde{n}_{l,\,\sigma}$ are given in \autoref{eq:new-n-flipping}.

\begin{equation}
    \label{eq:new-n-flipping}
    \tilde{n}_{l,\,\sigma} = \begin{cases}
        l = i \land \sigma = \sigma_i\text{ : }  &  (1 - n_{i,\,\sigma_i})  \\
        \text{else: }   &    n_{l,\,\sigma} 
    \end{cases}
\end{equation}

This simplifies the energy difference $E_0(N)-E_0(\vphantom{N}\smash{\tilde{N}})$ like equation \autoref{eq:simplified-base-energy-flipping} presents.

\begin{equation}
    \label{eq:simplified-base-energy-flipping}
    \begin{split}
        E_0(N)-E_0(\vphantom{N}\smash{\tilde{N}}) 
        &\stackrel{\phantom{\ref{eq:new-n-flipping}}}{=} U\lsum n_{l,\,\down}n_{l,\,\up}-U\lsum \tilde{n}_{l,\,\down}\tilde{n}_{l,\,\up} 
        + \lsum[l,\,\sigma] \epsl n_{l,\,\sigma} - \lsum[l,\,\sigma] \epsl \tilde{n}_{l,\,\sigma}\\
        &\stackrel{\ref{eq:new-n-flipping}}{=} \epsl[i] \left(2 n_{i,\,\sigma_i} - 1\right) +
        U\cdot \begin{cases}
            \sigma_i = \,\up\text{: }   & n_{i,\,\down} (2 n_{i,\,\up} - 1)\\
            \sigma_i = \,\down\text{: } & n_{i,\,\up} (2 n_{i,\,\down} - 1)  
        \end{cases}
    \end{split}
\end{equation}

The optimized calculation of $\HNOft - \HNOft[\vphantom{N}\smash{\tilde{N}}]$ or the derived $\Phi_l(N) - \Phi_l(\vphantom{N}\smash{\tilde{N}})$ is more involved to get correct.
Still the process is mathematically straight forward. 
For hints and further resources on how this was implemented, see \fullref{sec:implementation-details-script-generation}.
How the correctness of the optimizations is assured, is briefly touched in \fullref{sec:implementation-details-simplification-verifications}.
Finally, a generalized (if maybe not optimally efficient) method is outlined in \fullref{sec:theory-optimizations-geometry}.

\subsubsection*{Hopping, Swapping or Double-Flip Modification}

A possible Monte-Carlo-modification that conserves particle number is the change that occurs when a particle from a specific site and spin (occupation 1, then 0) is transferred to a different site and spin combination that currently is un-occupied (occupation 0, then 1).
This so-called \emph{hopping} event occurs in this example from site $i$ and spin $\sigma_i$ to the site $j$ and spin $\sigma_j$. 

The hopping is a subset of the \emph{swapping} modification, that occurs in the case of two occupation-number being exchanged.
The difference being, that while swapping two identical occupations is possible, for the hopping having exactly one particle on the relevant two sites is a pre-requirement.

Depending on the use-case this is a relevant difference, yet in case of calculation of the differences of effective Hamiltonians it is irrelevant: 
On comparison with e.g. \autoref{eq:form-heff-difference}, one can see that the term containing $\Delta = \HeffOft[\vphantom{N}\smash{\tilde{N}}]-\HeffOft[N]$ is multiplied with 0 in the cases where hopping is not possible.

Furthermore, the only way one could possibly get a contribution by the difference of effective Hamiltonians is in the case where $n_{i,\,\sigma_i} \neq n_{j,\,\sigma_j}$.
Because if they are the same, swapping doesn't change the state and a check can be performed beforehand, to save on computational resources (if $\ketN[\vphantom{N}\smash{\tilde{N}}] = \ketN[N]$, trivially  $\Delta = \HeffOft[\vphantom{N}\smash{\tilde{N}}]-\HeffOft[N] = 0$).

Thirdly, one could also think about an event, where simultaneously two occupation-numbers on two different site- / spin-indices are flipped, or \emph{double-flipping}.
In the case where hopping is possible, this is yet again the exact same operation (the 0 changes to a 1 and the 1 on the second site to a 0).
Differences are shown in \autoref{table:hopping-is-swapping}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{l|cc|cc|c} 
        \toprule
        Type     &  $n_{i,\,\sigma_i}$ & $n_{j,\,\sigma_j}$ &
                    $\tilde{n}_{i,\,\sigma_i}$ & $\tilde{n}_{j,\,\sigma_j}$ & $\Delta$  \\  
        \midrule 
        hopping  & 0 & 0    & \texttimes & \texttimes & \texttimes \\
                 & 0 & 1    &     1      &      0     & $\Delta_1$ \\
                 & 1 & 0    &     0      &      1     & $\Delta_2$ \\
                 & 1 & 1    & \texttimes & \texttimes & \texttimes \\
        \midrule   
        swapping & 0 & 0    &     0      &      0     &     0      \\
                 & 0 & 1    &     1      &      0     & $\Delta_1$ \\
                 & 1 & 0    &     0      &      1     & $\Delta_2$ \\
                 & 1 & 1    &     1      &      1     &     0      \\
        \midrule   
        double-  & 0 & 0    &     1      &      1     & $\Delta_3$ \\
        flipping & 0 & 1    &     1      &      0     & $\Delta_1$ \\
                 & 1 & 0    &     0      &      1     & $\Delta_2$ \\
                 & 1 & 1    &     0      &      0     & $\Delta_4$ \\
        \bottomrule
    \end{tabular}
    \vspace{0.5cm}
    \caption{
        Basic tabular depiction of what the three different modification-operations do to the occupation-numbers of a state. 
        For a difference of effective Hamiltonians $ \Delta $ a placeholder value is listed.
        Where the exact value is known to be zero this is noted, otherwise a variable is listed.
        The same variable indicates the same value, or simply put in these cases the methods are exchangeable.
    }
    \label{table:hopping-is-swapping}
\end{table}

For the case of swapping, \autoref{eq:simplified-base-energy-hopping} shows the base-energy difference $E_0(N)-E_0(\vphantom{N}\smash{\tilde{N}})$ - the double flipping implementation would be constructed analogously to \autoref{eq:simplified-base-energy-flipping}.

\begin{equation}
    \label{eq:simplified-base-energy-hopping}
    \begin{split}
        E_0(N)-E_0(\vphantom{N}\smash{\tilde{N}}) 
        & = U\lsum n_{l,\,\down}n_{l,\,\up}-U\lsum \tilde{n}_{l,\,\down}\tilde{n}_{l,\,\up} 
        + \lsum[l,\,\sigma] \epsl n_{l,\,\sigma} - \lsum[l,\,\sigma] \epsl \tilde{n}_{l,\,\sigma}\\
        & =  \left(\epsl[i]-\epsl[j]\right)\left(n_{i,\,\sigma_i} - n_{j,\,\sigma_j}\right) +
        U\cdot \begin{cases}
            \sigma_i = \sigma_j\text{ : } & \left(n_{i,\,\up}-n_{j,\,\up}\right) \left(n_{i,\,\down}-n_{j,\,\down}\right)\\
            \sigma_i \neq \sigma_j\text{ : } & \left(n_{i,\,\up}-n_{j,\,\down}\right) \left(n_{i,\,\down}-n_{j,\,\up}\right)
        \end{cases}\\
        & = \left(\epsl[i]-\epsl[j]\right)\left(n_{i,\,\sigma_i} - n_{j,\,\sigma_j}\right) +
        U \left(n_{i,\,\sigma_i}-n_{j,\,\sigma_j}\right) \left(n_{i,\,\overline{\sigma_i}}-n_{j,\,\overline{\sigma_j}}\right)
    \end{split}
\end{equation}

To get the simplest possible implementation for more complicated terms, is is advised to implement double-flipping first.
It is the simplest of the three and (with the correct pre-factors that restrict the starting occupation-numbers) can be used to implement the other two easily.
This verified \filepath{\cite{selfCode}}{/computation-scripts/compareobservables.py}.