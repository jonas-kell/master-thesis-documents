When I was picking a subject for and started working on my master thesis and submissions related to it, I could not have forseen the developments that would happen in the field of \emph{Artificial Intelligence} (AI) during the time until I would have to submit it.

While finishing work on my \emph{bachelor thesis} \cite{selfBachelorThesis} in October 2022, AI was in the middle of having major breakthroughs in image-processing and similar specialized tasks. 
A sudden spike in performance of such models could, among other factors, be attributed to the \emph{transformer} architecture, I happened to also have been doing research on in said Bachelor Thesis.

One mere month after this, AI hit the mainstream with the introduction of \emph{Chat-GPT} \cite{chatGPT} around the new Year of 2023.
The access to this rapidly empowering and yet established technology had so far mainly been gated by the required technological experience and powerful hardware. 
That changed, when the chatbot Chat-GPT was made available for the masses. 
Various image-generation-services followed in the same time frame, causing an immediate availability of generative AI for basically all types of media.

From one moment to the next, everybody on the planet who had access to an internet browser, could harness this technology in a simple web interface, without any prior technological knowledge. 
This quickly lead to a global AI-arms-race \cite{aiBoom}.


So into this situation one has to integrate this research. 
While doing my bachelor, AI was used in purpose-built research and large-scale industrial-applications.
By now, every outlet is attaching buzzwords like \emph{AI} to every product and research.

My bachelor thesis was likely run through a plagiarism software, as it has been standard for probably 10-15 years - my master thesis could very well be first run through an \emph{AI-detection software} (that is itself \glqq AI\grqq), even though such means of cheating are only \emph{possible} since 24 months, while classical plagiarism is as old as written word.
The fact that I develop my research in open-source repositories might even cause or at least increase the likelihood of such a tool flagging my work - chances are real, that this paragraph will be submitted into training data the moment I commit it.
That in itself causing the next pupil generating their report with AI to draw upon these words unknowingly.\\

Then the news hit just some weeks ago: the \emph{Nobel Prize in Physics, 2024} was awarded to \emph{J.\,J.\,Hopfield} and \emph{G.\,E.\,Hinton} \glqq for foundational discoveries and inventions that enable machine learning with artificial neural networks\grqq{} \cite{nobelPrizePhysics2024}.
Ironically, this was met with a huge controversy \cite{NobelPrizeControversy} in most social-media outlets - why? - \glqq AI is not physics\grqq.

It took the people - even the physicists themselves working in fields that do not directly touch AI - two years of exposure to main stream Chat-GPT, to forget that before the \emph{generative-AI-phase}, machine-learning was a highly scientific tool to advance major research breakthroughs.
This gets even more ironic, once one realizes that the original (most cited) publication of Hopfield \emph{\glqq Neural networks and physical systems with emergent collective computational abilities\grqq} \cite{basePaperMachineLearning}, attempted to unify neurobiological research with solid state physics. 

One decade ago in 2014, the Nobel laureate released \emph{\glqq Whatever Happened to Solid State Physics?\grqq} \cite{hopfieldConnectionToSolidStatePhysics}. 
Like a caricature, this text mirrors this very question \glqq Does \emph{X} belong to field \emph{Y}\grqq.
The letter makes the early developments of condensed-matter-physics a subject of discussion and shows the voices of 1958, that ask whether solid state physics should even be a subfield of physics (as it was more material-science like, as opposed to e.g. laser-physics).

The early subfield of condensed matter was in the 1960s and 1970s closely tied to (neuro-) biology. 
This very connection also leading to coining of the term \emph{neuron} in 1982 for the architectural design of the first \emph{Hopfield Networks} and \emph{Restricted Boltzmann Machines} (RBM), the precursors to modern machine-learning-architectures.
And the very \emph{computational dynamics} research that was criticized in 1979 to be \glqq no neurobiology\grqq, but won the Swartz-Prize for the new field of computational neurobiology in 2012.

Looking back onto this history, I can't help but being proud having the opportunity to try working on my own piece of condensed-matter-research at a university with as much solid-state-expertise as the \emph{University of Augsburg}.